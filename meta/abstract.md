# optimizationBenchmarking.org: A Technical Introduction

Optimization algorithms have become a standard tool in many application areas such as management, logistics, engineering, design, chemistry, and medicine. They provide close-to-optimal approximate solutions for computationally hard problems within feasible time. This field has grown and evolved for the past 50 years and has several top-level journals dedicated to it. Research in optimization is focused on reducing the algorithm runtime and increasing the result quality. For such research to succeed and publications to have true impact on the real world, we need to be able to

  - analyze the performance of an algorithm, to  
  - analyze the influence of different features of an optimization problem on its hardness, and to  
  - compare the performance of different algorithms in a fair and sound fashion.

This presentation is a technical introduction, mainly intended for an audience who is already familiar with optimization and benchmarking. It is relatively short (shorter than our [general introduction](https://github.com/optimizationBenchmarking/documentation-intro-slides) found at https://optimizationbenchmarking.github.io/introSlides.html) and centered around a hands-on example of how to use our software.

Many optimization methods are [anytime algorithms](https://en.wikipedia.org/wiki/Anytime_algorithm), meaning that they start with a (usually bad) guess about the solution and step-by-step improve their approximation quality. All [evolutionary algorithms](https://en.wikipedia.org/wiki/Evolutionary_algorithm), all [local search](https://en.wikipedia.org/wiki/Local_search_%28optimization%29) algorithms (such as [Simulated Annealing](https://en.wikipedia.org/wiki/Simulated_annealing) and [Tabu Search](https://en.wikipedia.org/wiki/Tabu_search)), all [swarm intelligence](https://en.wikipedia.org/wiki/Swarm_intelligence) methods for optimization (such as [ant colony](https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms) and [particle swarm](https://en.wikipedia.org/wiki/Particle_swarm_optimization) optimization), [CMA-ES](https://en.wikipedia.org/wiki/CMA-ES) and [memetic algorithms](https://en.wikipedia.org/wiki/Memetic_algorithm), but also several exact and deterministic methods such as [branch and bound](https://en.wikipedia.org/wiki/Branch_and_bound) belong into this class, just to name a few.

The comparison and evaluation of anytime algorithms must consider the whole runtime behavior of the algorithms in order to avoid misleading conclusions. Also, performance data has to be gathered from multiple independent runs on multiple different benchmark instances. It is easy to see that a thorough analysis and comparison of optimization algorithms is complicated and cumbersome. We present an open source software which can do this for you. You gather the data from your experiments, the software analyzes it. Our goal is to support researchers and practitioners as much as possible by automating the evaluation of experimental results. The software does not require any programming, just your benchmarking data. It imposes no limits, neither on the type of algorithms to be compared nor on the type of problem they are benchmarked on. Our software produces human-readable conclusions and reports in either XHTML or LaTeX format. You can freely select and configure different diagram types and group your data according to different aspects to get a better understanding of the behavior of your algorithm. Figures are styled for direct re-use in journals such as IEEE Transactions or conference proceedings such as LNCS. The software is dockerized, meaning that you can directly apply it with minimal installation effort.

We demonstrate the utility of this software on the [example](https://github.com/optimizationBenchmarking/documentation-examples/tree/gh-pages/data/maxSat) of the investigation of six primitive heuristics on the Maximum Satisfiability Problem (MAX-SAT). Similar examples are provided for download for [numerical optimization](https://github.com/optimizationBenchmarking/documentation-examples/tree/gh-pages/data/bbob) and the [Traveling Salesman Problem](https://github.com/optimizationBenchmarking/documentation-examples/tree/gh-pages/data/tspSuite).

More information can be found at http://optimizationbenchmarking.github.io/.

## References

* Thomas Weise, Raymond Chiong, Ke Tang, Jörg Lässig, Shigeyoshi Tsutsui, Wenxiang Chen, Zbigniew Michalewicz, and Xin Yao. Benchmarking Optimization Algorithms: An Open Source Framework for the Traveling Salesman Problem. <em>IEEE Computational Intelligence Magazine (CIM)</em>, 9(3):40-52, August&nbsp;2014. Featured article and selected paper at the website of the IEEE Computational Intelligence Society (<a href="http://cis.ieee.org/">http://cis.ieee.org/</a>).<br><a href="http://www.it-weise.de/research/publications/WCTLTCMY2014BOAAOSFFTTSP/index.html">details</a> / doi:<a href="http://dx.doi.org/10.1109/MCI.2014.2326101">10.1109/MCI.2014.2326101</a> / <a href="http://www.it-weise.de/research/publications/WCTLTCMY2014BOAAOSFFTTSP/WCTLTCMY2014BOAAOSFFTTSP.pdf">pdf</a>
* Thomas Weise, Yuezhong Wu, Raymond Chiong, Ke Tang, and Jörg Lässig. Global versus Local Search: The Impact of Population Sizes on Evolutionary Algorithm Performance. <em>Journal of Global Optimization</em>. accepted 12&nbsp;February, 2016, published first online: 23&nbsp;February, 2016.<br><a href="http://www.it-weise.de/research/publications/WWCTL2016GVLSTIOPSOEAP/index.html">details</a> / doi:<a href="http://dx.doi.org/10.1007/s10898-016-0417-5">10.1007/s10898-016-0417-5</a> / <a href="http://www.it-weise.de/research/publications/WWCTL2016GVLSTIOPSOEAP/WWCTL2016GVLSTIOPSOEAP.pdf">pdf</a>
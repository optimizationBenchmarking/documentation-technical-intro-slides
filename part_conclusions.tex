%
\section{Conclusions}%
%
\begin{frame}%
\frametitle{Conclusions}%
\begin{itemize}%
\item I have presented a very first version of the evaluator GUI component of the \optimizationBenchmarking\ framework%
\item<2-> It can already load and evaluate performance data from \emph{your} optimization or Machine Learning algorithm%
\item<3-> It can help \emph{you} to understand what the strengths and weaknesses of \emph{your} algorithm are%
\item<4-> It produces figures ready for use in \emph{your} publication%
\item<5-> {\dots}and these figures are optimized (size, fonts) for the journal or conference \emph{you} want to submit to.%
\item<6-> Btw, you could even compare general algorithms (like GAs and HC) on entirely different problem types at once (like MAX-SAT and BBOB) by making the problem type an instance feature\dots%  
\end{itemize}%
%%
\end{frame}
%
%
\begin{frame}%
\frametitle{Future Work: Short-Term}%
\begin{itemize}%
\item Add more evaluation modules, to reach the power of \tspSuite\expandafter\scitep{\tspSuiteReferences}, e.g., add automated algorithm ranking%
\item<2-> Publish overview paper about our system to publish it more widely\dots\ {\dots}when it's ready%
\item<3-> Publicize the use of \optimizationBenchmarking\ to our colleagues (citations will come in\dots)%
\item<4-> Position tool as a central quality control utility for optimization and Machine Learning applications%
\item<5-> Improve features based on feedback%
\end{itemize}%
%%
\end{frame}
%
\begin{frame}%
\frametitle{Future Work: Long-Term}%
\begin{itemize}%
\item Scout for new interesting ways to evaluate optimization and Machine Learning algorithms and implement them as evaluator modules%
\item<2-> In Progress: We could use clustering to group algorithms by their behavior or problems by their hardness%
\item<3-> Btw: This is Big Data, since we can collect \emph{much} information\dots%
\end{itemize}%
\end{frame}%
%